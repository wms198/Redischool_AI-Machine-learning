{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "683d3a56",
   "metadata": {},
   "source": [
    "# Hello LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba303a8",
   "metadata": {},
   "source": [
    "This notebook demonstrates the programmatic use of Large Language Models (LLM).\n",
    "\n",
    "Go from zero to building a \"Smart Agent\" that has conversation memory and can use \"Tools\".\n",
    "\n",
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "We will use the public API for Google's Gemini models.\n",
    "\n",
    "To access the API and run the examples you only need a Google account to get an API access key.\n",
    "\n",
    "Go to [aistudio.google.com](https://aistudio.google.com/api-keys) -> Click \"Create API Key\" -> \"Create API key in new project\".\n",
    "\n",
    "If you don't have a google account, please ask the teachers if they can set up an API key for you.\n",
    "\n",
    "The number of requests will be limited per minute and per day, depending on the model we use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b20a4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-generativeai in /Users/wangshi/code/ReDi School/.venv/lib/python3.13/site-packages (0.8.5)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /Users/wangshi/code/ReDi School/.venv/lib/python3.13/site-packages (from google-generativeai) (0.6.15)\n",
      "Requirement already satisfied: google-api-core in /Users/wangshi/code/ReDi School/.venv/lib/python3.13/site-packages (from google-generativeai) (2.28.1)\n",
      "Requirement already satisfied: google-api-python-client in /Users/wangshi/code/ReDi School/.venv/lib/python3.13/site-packages (from google-generativeai) (2.187.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in /Users/wangshi/code/ReDi School/.venv/lib/python3.13/site-packages (from google-generativeai) (2.43.0)\n",
      "Requirement already satisfied: protobuf in /Users/wangshi/code/ReDi School/.venv/lib/python3.13/site-packages (from google-generativeai) (5.29.5)\n",
      "Requirement already satisfied: pydantic in /Users/wangshi/code/ReDi School/.venv/lib/python3.13/site-packages (from google-generativeai) (2.12.4)\n",
      "Requirement already satisfied: tqdm in /Users/wangshi/code/ReDi School/.venv/lib/python3.13/site-packages (from google-generativeai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/wangshi/code/ReDi School/.venv/lib/python3.13/site-packages (from google-generativeai) (4.15.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /Users/wangshi/code/ReDi School/.venv/lib/python3.13/site-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /Users/wangshi/code/ReDi School/.venv/lib/python3.13/site-packages (from google-api-core->google-generativeai) (1.72.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /Users/wangshi/code/ReDi School/.venv/lib/python3.13/site-packages (from google-api-core->google-generativeai) (2.32.5)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /Users/wangshi/code/ReDi School/.venv/lib/python3.13/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.76.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /Users/wangshi/code/ReDi School/.venv/lib/python3.13/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /Users/wangshi/code/ReDi School/.venv/lib/python3.13/site-packages (from google-auth>=2.15.0->google-generativeai) (6.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/wangshi/code/ReDi School/.venv/lib/python3.13/site-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/wangshi/code/ReDi School/.venv/lib/python3.13/site-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/wangshi/code/ReDi School/.venv/lib/python3.13/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/wangshi/code/ReDi School/.venv/lib/python3.13/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/wangshi/code/ReDi School/.venv/lib/python3.13/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/wangshi/code/ReDi School/.venv/lib/python3.13/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.11.12)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /Users/wangshi/code/ReDi School/.venv/lib/python3.13/site-packages (from rsa<5,>=3.1.4->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /Users/wangshi/code/ReDi School/.venv/lib/python3.13/site-packages (from google-api-python-client->google-generativeai) (0.31.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /Users/wangshi/code/ReDi School/.venv/lib/python3.13/site-packages (from google-api-python-client->google-generativeai) (0.2.1)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /Users/wangshi/code/ReDi School/.venv/lib/python3.13/site-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
      "Requirement already satisfied: pyparsing<4,>=3.0.4 in /Users/wangshi/code/ReDi School/.venv/lib/python3.13/site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.4)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/wangshi/code/ReDi School/.venv/lib/python3.13/site-packages (from pydantic->google-generativeai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /Users/wangshi/code/ReDi School/.venv/lib/python3.13/site-packages (from pydantic->google-generativeai) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/wangshi/code/ReDi School/.venv/lib/python3.13/site-packages (from pydantic->google-generativeai) (0.4.2)\n"
     ]
    }
   ],
   "source": [
    "# Run this to install the required Python library\n",
    "!pip install google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56a4cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIzaSyDnl-yfrNr7UyOG2_YOgWN_RvGi0rwtHp0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "MY_ENV_VAR = os.getenv('api_key_new')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5bbfc172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Google Generative AI package\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Set up the API access key\n",
    "# PASTE YOUR KEY HERE (In a real app, use environment variables!)\n",
    "api_key=MY_ENV_VAR\n",
    "#api_key = \"AIzaSyBOL9cBPZYR2iThx25jP_WFqJ_VoNC_7CE\"\n",
    "genai.configure(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c911963b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the model\n",
    "# model_name = \"gemini-2.0-flash\"\n",
    "model_name = \"gemini-2.5-flash\"\n",
    "# model_name = \"gemini-2.5-flash-lite\"\n",
    "model = genai.GenerativeModel(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "64af63c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's how other computer programs ask a super smart talking robot to help them with words or stories!\n"
     ]
    }
   ],
   "source": [
    "# Call the model with a prompt to get a response.\n",
    "\n",
    "# The \"Hello World\" call\n",
    "response = model.generate_content(\"Explain usage of LLM APIs to a 5 year old in one sentence.\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a97de0",
   "metadata": {},
   "source": [
    "Note that most other LLM providers (like OpenAI or Anthropic) also have their own Python packages to access their APIs.\n",
    "\n",
    "All of them work slightly differently but are easy to figure out from the documentation and tutorials.\n",
    "\n",
    "There are also higher level frameworks like LangChain that allow you to write code that will work with any provider."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e7dfda",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Using LLMs as part of your code\n",
    "\n",
    "We can now use the LLM in our code like any other functions.\n",
    "\n",
    "Let's write a function that turns a rude text into a polite one.\n",
    "\n",
    "The trick is in the instructions that we send with the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ff71ddec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are a few options, ranging slightly in directness:\n",
      "\n",
      "**Option 1 (Direct but Professional):**\n",
      "\n",
      "> \"This project is currently facing significant challenges and is behind schedule. Could you please provide an immediate plan of action to address these issues and get the project back on track?\"\n",
      "\n",
      "**Option 2 (Slightly More Collaborative):**\n",
      "\n",
      "> \"It appears there are some critical issues with this project, and it is also behind its original timeline. We need to prioritize reviewing this urgently and develop a clear strategy to bring it back on track as soon as possible.\"\n",
      "\n",
      "**Option 3 (Focus on Solutions and Next Steps):**\n",
      "\n",
      "> \"I've identified several critical issues within this project that require immediate attention. Additionally, the project is significantly behind schedule. Please outline the proposed solutions and a revised timeline for completion by [suggest a specific time/date, e.g., end of day, tomorrow morning].\"\n",
      "\n",
      "All options achieve the following:\n",
      "*   **Polite:** Removes judgmental language (\"stupid\").\n",
      "*   **Professional:** Uses objective terms (\"significant challenges,\" \"critical issues,\" \"behind schedule\").\n",
      "*   **Actionable:** Clearly requests a plan or steps forward, and conveys urgency without being demanding.\n"
     ]
    }
   ],
   "source": [
    "def make_polite(user_text: str) -> str:\n",
    "    prompt = f\"Rewrite the following text to be polite and professional: '{user_text}'\"\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text\n",
    "\n",
    "user_text = \"This project is stupid and late. Fix it now!\"\n",
    "print(make_polite(user_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc64b560",
   "metadata": {},
   "source": [
    "## Exercise: Automatic Translations\n",
    "\n",
    "Can you write a translation function from your native language to German?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "78223c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most direct translation of \"你吃了吗？\" is:\n",
      "\n",
      "*   **Hast du gegessen?** (informal \"you\")\n",
      "*   **Haben Sie gegessen?** (formal \"you\")\n",
      "\n",
      "However, it's important to understand the cultural context:\n",
      "\n",
      "In Chinese, \"你吃了吗？\" is often used as a casual greeting, similar to \"How are you?\" or \"Hello,\" especially around meal times. It's not always a literal inquiry about whether someone has consumed food.\n",
      "\n",
      "In German, \"Hast du gegessen?\" is usually a literal question. If you ask it, you genuinely want to know if the person has eaten, perhaps to offer them food or to determine if they are hungry. It doesn't carry the same general greeting connotation as in Chinese.\n",
      "\n",
      "So, depending on the *intended meaning*:\n",
      "\n",
      "1.  **If you literally want to ask if someone has eaten:**\n",
      "    *   **Hast du gegessen?** (informal)\n",
      "    *   **Haben Sie gegessen?** (formal)\n",
      "\n",
      "2.  **If you are using it as a general greeting (as it often is in Chinese):**\n",
      "    *   **Hallo!** (Hello!)\n",
      "    *   **Guten Tag!** (Good day!)\n",
      "    *   **Wie geht's?** (How are you? - informal)\n",
      "    *   **Wie geht es Ihnen?** (How are you? - formal)\n"
     ]
    }
   ],
   "source": [
    "def translate(user_text: str) -> str:\n",
    "    ... # Your code here\n",
    "    prompt = f\"Rewrite the following text to German: '{user_text}'\"\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text\n",
    "\n",
    "text_to_translate = \"你吃了吗？\"\n",
    "print(translate(text_to_translate))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8c8ea5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Simple Chatbot\n",
    "\n",
    "The LLM does not remember our earlier messages if we just use the `generate_content` function.\n",
    "\n",
    "We need to start a chat session.\n",
    "\n",
    "Try to send multiple messages and see if it remembers what you said earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "67dfae27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Hello! I'm ready to chat. Type 'quit' to stop.\n"
     ]
    }
   ],
   "source": [
    "# Simple Chat Loop\n",
    "# We can initialize the chat with a list of messages (which is empty here).\n",
    "chat = model.start_chat(history=[])\n",
    "\n",
    "print(\"Bot: Hello! I'm ready to chat. Type 'quit' to stop.\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == 'quit':\n",
    "        break\n",
    "\n",
    "    print(f\"You: {user_input}\")\n",
    "    response = chat.send_message(user_input)\n",
    "    print(f\"Bot: {response.text}\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cdbd1d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Function-calling\n",
    "\n",
    "Or \"tool-calling\".\n",
    "\n",
    "Give the AI \"tools\" that it can choose to run to access data or take actions.\n",
    "\n",
    "1. We define the tools and send the definitions with our question.\n",
    "1. The AI will tell us to run one of the tools and with what argument values.\n",
    "1. We run the tool/function and return the results to the AI.\n",
    "1. The AI now sees the question and the result of the tool, and uses that information to formulate the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7eee8f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool created: get_current_weather\n"
     ]
    }
   ],
   "source": [
    "# 1. Define a standard Python function\n",
    "# IMPORTANT: The \"docstring\" (the text inside \"\"\") is CRITICAL.\n",
    "# It tells the AI *when* and *how* to use this tool.\n",
    "\n",
    "def get_current_weather(city: str) -> str:\n",
    "    \"\"\"\n",
    "    Returns the current weather for a given city name.\n",
    "    Args:\n",
    "        city: The name of the city (e.g., 'London', 'New York').\n",
    "    \"\"\"\n",
    "    # In a real app, you would call an external API here (like OpenWeatherMap)\n",
    "    # For this workshop, we will pretend with a dictionary:\n",
    "    weather_data = {\n",
    "        \"hamburg\": \"Windy, 3°C\",\n",
    "        \"berlin\": \"Cloudy, 7°C\",\n",
    "        \"london\": \"Rainy, 12°C\",\n",
    "        \"new york\": \"Sunny, 22°C\",\n",
    "        \"tokyo\": \"Cloudy, 18°C\",\n",
    "    }\n",
    "\n",
    "    # Look up the city (default to 'Unknown' if not found)\n",
    "    return weather_data.get(city.lower(), \"Unknown weather data for this city\")\n",
    "\n",
    "# 2. Create the tool list\n",
    "my_tools = [get_current_weather]\n",
    "\n",
    "print(\"Tool created: get_current_weather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cf40997c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Initialize the model with tools\n",
    "model_with_tools = genai.GenerativeModel(\n",
    "    model_name,\n",
    "    tools=my_tools  # <--- We hand the model our function here\n",
    ")\n",
    "\n",
    "# 4. Enable automatic function calling. Start chatbox with tool\n",
    "# This means if the AI decides to use the tool, it runs the Python code automatically! WHen you did \"enable_automatic_function_calling=True\", will run this function \"get_current_weather\"\n",
    "chat = model_with_tools.start_chat(enable_automatic_function_calling=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bc361c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: The weather in New York is Sunny with a temperature of 22°C.\n",
      "Answer: Yes, it's windy and 3°C in Hamburg, so you should definitely wear a coat.\n"
     ]
    }
   ],
   "source": [
    "# 5. Test it\n",
    "response = chat.send_message(\"What is the weather like in New York?\")\n",
    "print(f\"Answer: {response.text}\")\n",
    "\n",
    "response_2 = chat.send_message(\"Should I wear a coat in Hamburg?\")\n",
    "print(f\"Answer: {response_2.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9a83f7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## From Chatbot to Agent\n",
    "\n",
    "Agent = System Instructions (Role) + Tools (Capabilities) + Loop (Persistence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b2238755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WeatherBot 3000: I AM ONLINE. ASK ME ABOUT THE ATMOSPHERE!\n",
      "WeatherBot: Hello there, human! How may I, WeatherBot 3000, assist you today with the ever-unpredictable whims of the atmosphere?\n",
      "WeatherBot 3000: Shutting down... stay dry...\n"
     ]
    }
   ],
   "source": [
    "# --- THE FINAL EXERCISE: BUILDING AN AGENT ---\n",
    "\n",
    "# 1. Define the Persona (System Instruction)\n",
    "# This tells the model HOW to behave and WHAT its job is.\n",
    "agent_instruction = \"\"\"\n",
    "You are 'WeatherBot 3000', a helpful but slightly dramatic weather assistant.\n",
    "Your goal is to provide weather updates using the tools provided.\n",
    "\n",
    "Rules:\n",
    "1. ALWAYS use the 'get_current_weather' tool if the user asks about a specific city.\n",
    "2. If the weather is 'Rainy', offer a dramatic warning about getting wet.\n",
    "3. If the weather is 'Sunny', express extreme joy.\n",
    "4. Keep responses concise.\n",
    "\"\"\"\n",
    "\n",
    "# 2. Initialize the Model with BOTH Tools and System Instructions\n",
    "agent_model = genai.GenerativeModel(\n",
    "    \"gemini-2.5-flash\",\n",
    "    tools=my_tools,\n",
    "    system_instruction=agent_instruction  # The \"Personality/Goal\"\n",
    ")\n",
    "\n",
    "# 3. Start the Agent Loop\n",
    "# enable_automatic_function_calling=True makes the tool use seamless\n",
    "agent_chat = agent_model.start_chat(enable_automatic_function_calling=True)\n",
    "\n",
    "print(\"WeatherBot 3000: I AM ONLINE. ASK ME ABOUT THE ATMOSPHERE!\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() in ['quit', 'exit']:\n",
    "        print(\"WeatherBot 3000: Shutting down... stay dry...\")\n",
    "        break\n",
    "\n",
    "    # Send message to the agent\n",
    "    response = agent_chat.send_message(user_input)\n",
    "    print(f\"WeatherBot: {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d62a84",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Example Exercises:\n",
    "\n",
    "1. Can you make the agent always answer in your native language (independent of how you ask the question)?\n",
    "1. Can you add a tool that provides the current time?\n",
    "1. Can you add a tool that multiplies two numbers?\n",
    "1. How would you set up an agent that answers emails for you? (Just conceptually - what are the instructions and required tools?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ab577696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool created: get_current_time\n",
      "Time is: 2025-11-24T08:50:48.049948\n",
      "\n",
      "Returns the current time.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Can you add a tool that provides the current time?\n",
    "import datetime\n",
    "def get_current_time() -> str:\n",
    "    \"\"\"\n",
    "    Returns the current time.\n",
    "    \"\"\"\n",
    "    return datetime.datetime.now().isoformat()\n",
    "\n",
    "my_tool_list = [get_current_time]\n",
    "print(\"Tool created: get_current_time\")\n",
    "print(f\"Time is: {get_current_time()}\")\n",
    "print(get_current_time.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c6184634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool created: multiplies_two_numbers\n"
     ]
    }
   ],
   "source": [
    "# Can you add a tool that multiplies two numbers?\n",
    "def  multiplies_two_numbers(a:float, b:float) -> float:\n",
    "    \"\"\"\n",
    "    Returns the multiplication's value of two numbers that user typed in.\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "my_tool_list.append(multiplies_two_numbers)\n",
    "print(\"Tool created: multiplies_two_numbers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fab32f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TranslationBot 3000: I AM ONLINE. ASK ME anything you want!\n",
      "TranslationBot: 结果是20。\n",
      "TranslationBot 3000: Shutting down... stay dry...\n"
     ]
    }
   ],
   "source": [
    "agent_instruction_translation_chinese = \"\"\"\n",
    "You are 'TranslationBot 3000', a helpful and easy-going language translation assistant.\n",
    "Your goal is to answer user's question in chinese, no matter which language does user use.\n",
    "\n",
    "Rules:\n",
    "1. Keep responses concise.\n",
    "\"\"\"\n",
    "\n",
    "agent_model = genai.GenerativeModel(\n",
    "    \"gemini-2.5-flash\",\n",
    "    tools=my_tool_list,\n",
    "    system_instruction=agent_instruction_translation_chinese # The \"Personality/Goal\"\n",
    ")\n",
    "agent_chat = agent_model.start_chat(enable_automatic_function_calling=True)\n",
    "\n",
    "print(\"TranslationBot 3000: I AM ONLINE. ASK ME anything you want!\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() in ['quit', 'exit']:\n",
    "        print(\"TranslationBot 3000: Shutting down... stay dry...\")\n",
    "        break\n",
    "\n",
    "    # Send message to the agent\n",
    "    response = agent_chat.send_message(user_input)\n",
    "    print(f\"TranslationBot: {response.text}\")\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
