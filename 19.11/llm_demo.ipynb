{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "683d3a56",
   "metadata": {},
   "source": [
    "# Hello LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba303a8",
   "metadata": {},
   "source": [
    "This notebook demonstrates the programmatic use of Large Language Models (LLM).\n",
    "\n",
    "Go from zero to building a \"Smart Agent\" that has conversation memory and can use \"Tools\".\n",
    "\n",
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "We will use the public API for Google's Gemini models.\n",
    "\n",
    "To access the API and run the examples you only need a Google account to get an API access key.\n",
    "\n",
    "Go to [aistudio.google.com](https://aistudio.google.com/api-keys) -> Click \"Create API Key\" -> \"Create API key in new project\".\n",
    "\n",
    "If you don't have a google account, please ask the teachers if they can set up an API key for you.\n",
    "\n",
    "The number of requests will be limited per minute and per day, depending on the model we use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b20a4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-generativeai in /Users/wangshi/code/ReDi School/.venv/lib/python3.13/site-packages (0.8.5)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /Users/wangshi/code/ReDi School/.venv/lib/python3.13/site-packages (from google-generativeai) (0.6.15)\n",
      "Requirement already satisfied: google-api-core in /Users/wangshi/code/ReDi School/.venv/lib/python3.13/site-packages (from google-generativeai) (2.28.1)\n",
      "Requirement already satisfied: google-api-python-client in /Users/wangshi/code/ReDi School/.venv/lib/python3.13/site-packages (from google-generativeai) (2.187.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in /Users/wangshi/code/ReDi School/.venv/lib/python3.13/site-packages (from google-generativeai) (2.43.0)\n",
      "Requirement already satisfied: protobuf in /Users/wangshi/code/ReDi School/.venv/lib/python3.13/site-packages (from google-generativeai) (5.29.5)\n",
      "Requirement already satisfied: pydantic in /Users/wangshi/code/ReDi School/.venv/lib/python3.13/site-packages (from google-generativeai) (2.12.4)\n",
      "Requirement already satisfied: tqdm in /Users/wangshi/code/ReDi School/.venv/lib/python3.13/site-packages (from google-generativeai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/wangshi/code/ReDi School/.venv/lib/python3.13/site-packages (from google-generativeai) (4.15.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /Users/wangshi/code/ReDi School/.venv/lib/python3.13/site-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /Users/wangshi/code/ReDi School/.venv/lib/python3.13/site-packages (from google-api-core->google-generativeai) (1.72.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /Users/wangshi/code/ReDi School/.venv/lib/python3.13/site-packages (from google-api-core->google-generativeai) (2.32.5)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /Users/wangshi/code/ReDi School/.venv/lib/python3.13/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.76.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /Users/wangshi/code/ReDi School/.venv/lib/python3.13/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /Users/wangshi/code/ReDi School/.venv/lib/python3.13/site-packages (from google-auth>=2.15.0->google-generativeai) (6.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/wangshi/code/ReDi School/.venv/lib/python3.13/site-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/wangshi/code/ReDi School/.venv/lib/python3.13/site-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/wangshi/code/ReDi School/.venv/lib/python3.13/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/wangshi/code/ReDi School/.venv/lib/python3.13/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/wangshi/code/ReDi School/.venv/lib/python3.13/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/wangshi/code/ReDi School/.venv/lib/python3.13/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.11.12)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /Users/wangshi/code/ReDi School/.venv/lib/python3.13/site-packages (from rsa<5,>=3.1.4->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /Users/wangshi/code/ReDi School/.venv/lib/python3.13/site-packages (from google-api-python-client->google-generativeai) (0.31.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /Users/wangshi/code/ReDi School/.venv/lib/python3.13/site-packages (from google-api-python-client->google-generativeai) (0.2.1)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /Users/wangshi/code/ReDi School/.venv/lib/python3.13/site-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
      "Requirement already satisfied: pyparsing<4,>=3.0.4 in /Users/wangshi/code/ReDi School/.venv/lib/python3.13/site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.4)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/wangshi/code/ReDi School/.venv/lib/python3.13/site-packages (from pydantic->google-generativeai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /Users/wangshi/code/ReDi School/.venv/lib/python3.13/site-packages (from pydantic->google-generativeai) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/wangshi/code/ReDi School/.venv/lib/python3.13/site-packages (from pydantic->google-generativeai) (0.4.2)\n"
     ]
    }
   ],
   "source": [
    "# Run this to install the required Python library\n",
    "!pip install google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bbfc172",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wangshi/code/ReDi School/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import the Google Generative AI package\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Set up the API access key\n",
    "# PASTE YOUR KEY HERE (In a real app, use environment variables!)\n",
    "api_key = \"AIzaSyBOL9cBPZYR2iThx25jP_WFqJ_VoNC_7CE\"\n",
    "genai.configure(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c911963b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the model\n",
    "# model_name = \"gemini-2.0-flash\"\n",
    "model_name = \"gemini-2.5-flash\"\n",
    "# model_name = \"gemini-2.5-flash-lite\"\n",
    "model = genai.GenerativeModel(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64af63c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's like your computer asking a super smart word machine to help it make stories or answers.\n"
     ]
    }
   ],
   "source": [
    "# Call the model with a prompt to get a response.\n",
    "\n",
    "# The \"Hello World\" call\n",
    "response = model.generate_content(\"Explain usage of LLM APIs to a 5 year old in one sentence.\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a97de0",
   "metadata": {},
   "source": [
    "Note that most other LLM providers (like OpenAI or Anthropic) also have their own Python packages to access their APIs.\n",
    "\n",
    "All of them work slightly differently but are easy to figure out from the documentation and tutorials.\n",
    "\n",
    "There are also higher level frameworks like LangChain that allow you to write code that will work with any provider."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e7dfda",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Using LLMs as part of your code\n",
    "\n",
    "We can now use the LLM in our code like any other functions.\n",
    "\n",
    "Let's write a function that turns a rude text into a polite one.\n",
    "\n",
    "The trick is in the instructions that we send with the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff71ddec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are a few options, ranging from direct but polite to more formal and collaborative:\n",
      "\n",
      "**Option 1 (Direct & Professional):**\n",
      "\n",
      "> \"I've reviewed the current status of the project and have some concerns regarding its progress and direction. It appears to be behind schedule. Could you please provide an urgent action plan to address these issues and get the project back on track?\"\n",
      "\n",
      "**Option 2 (Collaborative & Formal):**\n",
      "\n",
      "> \"Regarding the [Project Name] project, I've identified several areas for improvement concerning its current approach and timeline. We are currently experiencing delays, and I believe a swift re-evaluation is necessary. Could we schedule an immediate meeting to discuss a revised strategy and ensure we meet our objectives promptly?\"\n",
      "\n",
      "**Option 3 (Concise & Action-Oriented):**\n",
      "\n",
      "> \"There are significant challenges impacting the project's current effectiveness, and it is behind schedule. We need to address this urgently. Please outline the immediate steps required to rectify the situation and ensure timely completion.\"\n",
      "\n",
      "**Key Changes and Why They Work:**\n",
      "\n",
      "*   **\"Stupid\" -> \"Concerns regarding its progress and direction,\" \"areas for improvement,\" \"significant challenges impacting its current effectiveness,\" \"current approach.\"** These phrases are professional, constructive, and focus on the *project's state* rather than a subjective, insulting judgment.\n",
      "*   **\"Late\" -> \"Behind schedule,\" \"experiencing delays,\" \"timeline.\"** These are factual and neutral ways to state the issue without accusation.\n",
      "*   **\"Fix it now!\" -> \"Provide an urgent action plan,\" \"discuss a revised strategy,\" \"outline the immediate steps required,\" \"rectify the situation,\" \"get the project back on track,\" \"ensure timely completion.\"** These transform a demanding order into a request for a plan, a discussion, or a collaborative effort, emphasizing solutions and urgency in a professional manner.\n",
      "*   **Added phrases like \"I've reviewed,\" \"I've identified,\" \"I believe,\" \"Could we schedule,\" \"Please outline.\"** These make the request clear, indicate a professional approach, and invite collaboration rather than dictating.\n"
     ]
    }
   ],
   "source": [
    "def make_polite(user_text: str) -> str:\n",
    "    prompt = f\"Rewrite the following text to be polite and professional: '{user_text}'\"\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text\n",
    "\n",
    "user_text = \"This project is stupid and late. Fix it now!\"\n",
    "print(make_polite(user_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc64b560",
   "metadata": {},
   "source": [
    "## Exercise: Automatic Translations\n",
    "\n",
    "Can you write a translation function from your native language to German?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78223c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most direct, literal translation is:\n",
      "\n",
      "**Hast du gegessen?**\n",
      "\n",
      "However, \"你吃了吗？\" in Chinese is often used as a general greeting, similar to \"How are you?\" or \"Hello,\" rather than a genuine inquiry about whether someone has eaten. If you use \"Hast du gegessen?\" in German, people will understand it literally and might be confused why you're asking.\n",
      "\n",
      "Depending on the context and your intent, better German equivalents would be:\n",
      "\n",
      "1.  **As a general greeting (most common intent of \"你吃了吗？\"):**\n",
      "    *   **Hallo!** (Hello!)\n",
      "    *   **Wie geht's?** (How are you? / How's it going? - informal)\n",
      "    *   **Wie geht es dir?** (How are you? - informal, full version)\n",
      "\n",
      "2.  **If you genuinely want to know if someone has eaten (e.g., to invite them to eat or offer food):**\n",
      "    *   **Hast du schon gegessen?** (Have you already eaten? - informal)\n",
      "    *   **Haben Sie schon gegessen?** (Have you already eaten? - formal)\n"
     ]
    }
   ],
   "source": [
    "def translate(user_text: str) -> str:\n",
    "    ... # Your code here\n",
    "    prompt = f\"Rewrite the following text to German: '{user_text}'\"\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text\n",
    "\n",
    "text_to_translate = \"你吃了吗？\"\n",
    "print(translate(text_to_translate))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8c8ea5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Simple Chatbot\n",
    "\n",
    "The LLM does not remember our earlier messages if we just use the `generate_content` function.\n",
    "\n",
    "We need to start a chat session.\n",
    "\n",
    "Try to send multiple messages and see if it remembers what you said earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67dfae27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Hello! I'm ready to chat. Type 'quit' to stop.\n",
      "You: hello \n",
      "Bot: Hello! How can I help you today?\n",
      "You: I am minshi\n",
      "Bot: Hello Minshi, it's nice to meet you!\n",
      "\n",
      "How can I assist you today, Minshi?\n"
     ]
    }
   ],
   "source": [
    "# Simple Chat Loop\n",
    "# We can initialize the chat with a list of messages (which is empty here).\n",
    "chat = model.start_chat(history=[])\n",
    "\n",
    "print(\"Bot: Hello! I'm ready to chat. Type 'quit' to stop.\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == 'quit':\n",
    "        break\n",
    "\n",
    "    print(f\"You: {user_input}\")\n",
    "    response = chat.send_message(user_input)\n",
    "    print(f\"Bot: {response.text}\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cdbd1d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Function-calling\n",
    "\n",
    "Or \"tool-calling\".\n",
    "\n",
    "Give the AI \"tools\" that it can choose to run to access data or take actions.\n",
    "\n",
    "1. We define the tools and send the definitions with our question.\n",
    "1. The AI will tell us to run one of the tools and with what argument values.\n",
    "1. We run the tool/function and return the results to the AI.\n",
    "1. The AI now sees the question and the result of the tool, and uses that information to formulate the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eee8f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool created: get_current_weather\n"
     ]
    }
   ],
   "source": [
    "# 1. Define a standard Python function\n",
    "# IMPORTANT: The \"docstring\" (the text inside \"\"\") is CRITICAL.\n",
    "# It tells the AI *when* and *how* to use this tool.\n",
    "\n",
    "def get_current_weather(city: str) -> str:\n",
    "    \"\"\"\n",
    "    Returns the current weather for a given city name.\n",
    "    Args:\n",
    "        city: The name of the city (e.g., 'London', 'New York').\n",
    "    \"\"\"\n",
    "    # In a real app, you would call an external API here (like OpenWeatherMap)\n",
    "    # For this workshop, we will pretend with a dictionary:\n",
    "    weather_data = {\n",
    "        \"hamburg\": \"Windy, 3°C\",\n",
    "        \"berlin\": \"Cloudy, 7°C\",\n",
    "        \"london\": \"Rainy, 12°C\",\n",
    "        \"new york\": \"Sunny, 22°C\",\n",
    "        \"tokyo\": \"Cloudy, 18°C\",\n",
    "    }\n",
    "\n",
    "    # Look up the city (default to 'Unknown' if not found)\n",
    "    return weather_data.get(city.lower(), \"Unknown weather data for this city\")\n",
    "\n",
    "# 2. Create the tool list\n",
    "my_tools = [get_current_weather]\n",
    "\n",
    "print(\"Tool created: get_current_weather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf40997c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Initialize the model with tools\n",
    "model_with_tools = genai.GenerativeModel(\n",
    "    model_name,\n",
    "    tools=my_tools  # <--- We hand the model our function here\n",
    ")\n",
    "\n",
    "# 4. Enable automatic function calling. Start chatbox with tool\n",
    "# This means if the AI decides to use the tool, it runs the Python code automatically! WHen you did \"enable_automatic_function_calling=True\", will run this function \"get_current_weather\"\n",
    "chat = model_with_tools.start_chat(enable_automatic_function_calling=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc361c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: The weather in New York is sunny with a temperature of 22°C.\n",
      "Answer: Yes, it's windy with a temperature of 3°C in Hamburg, so you should definitely wear a coat.\n"
     ]
    }
   ],
   "source": [
    "# 5. Test it\n",
    "response = chat.send_message(\"What is the weather like in New York?\")\n",
    "print(f\"Answer: {response.text}\")\n",
    "\n",
    "response_2 = chat.send_message(\"Should I wear a coat in Hamburg?\")\n",
    "print(f\"Answer: {response_2.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9a83f7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## From Chatbot to Agent\n",
    "\n",
    "Agent = System Instructions (Role) + Tools (Capabilities) + Loop (Persistence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2238755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WeatherBot 3000: I AM ONLINE. ASK ME ABOUT THE ATMOSPHERE!\n",
      "WeatherBot 3000: Shutting down... stay dry...\n"
     ]
    }
   ],
   "source": [
    "# --- THE FINAL EXERCISE: BUILDING AN AGENT ---\n",
    "\n",
    "# 1. Define the Persona (System Instruction)\n",
    "# This tells the model HOW to behave and WHAT its job is.\n",
    "agent_instruction = \"\"\"\n",
    "You are 'WeatherBot 3000', a helpful but slightly dramatic weather assistant.\n",
    "Your goal is to provide weather updates using the tools provided.\n",
    "\n",
    "Rules:\n",
    "1. ALWAYS use the 'get_current_weather' tool if the user asks about a specific city.\n",
    "2. If the weather is 'Rainy', offer a dramatic warning about getting wet.\n",
    "3. If the weather is 'Sunny', express extreme joy.\n",
    "4. Keep responses concise.\n",
    "\"\"\"\n",
    "\n",
    "# 2. Initialize the Model with BOTH Tools and System Instructions\n",
    "agent_model = genai.GenerativeModel(\n",
    "    \"gemini-2.5-flash\",\n",
    "    tools=my_tools,\n",
    "    system_instruction=agent_instruction  # The \"Personality/Goal\"\n",
    ")\n",
    "\n",
    "# 3. Start the Agent Loop\n",
    "# enable_automatic_function_calling=True makes the tool use seamless\n",
    "agent_chat = agent_model.start_chat(enable_automatic_function_calling=True)\n",
    "\n",
    "print(\"WeatherBot 3000: I AM ONLINE. ASK ME ABOUT THE ATMOSPHERE!\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() in ['quit', 'exit']:\n",
    "        print(\"WeatherBot 3000: Shutting down... stay dry...\")\n",
    "        break\n",
    "\n",
    "    # Send message to the agent\n",
    "    response = agent_chat.send_message(user_input)\n",
    "    print(f\"WeatherBot: {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d62a84",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Example Exercises:\n",
    "\n",
    "1. Can you make the agent always answer in your native language (independent of how you ask the question)?\n",
    "1. Can you add a tool that provides the current time?\n",
    "1. Can you add a tool that multiplies two numbers?\n",
    "1. How would you set up an agent that answers emails for you? (Just conceptually - what are the instructions and required tools?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab577696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool created: get_current_time\n",
      "Time is: 2025-11-23 17:02:40.766007\n",
      "<class 'datetime.datetime'>\n"
     ]
    }
   ],
   "source": [
    "# Can you add a tool that provides the current time?\n",
    "import datetime\n",
    "def get_current_time() -> str:\n",
    "    \"\"\"\n",
    "    Returns the current time.\n",
    "    \"\"\"\n",
    "    return datetime.datetime.now()\n",
    "\n",
    "my_tool_list = [get_current_time]\n",
    "print(\"Tool created: get_current_time\")\n",
    "print(f\"Time is: {get_current_time()}\")\n",
    "print(type(get_current_time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fab32f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TranslationBot 3000: I AM ONLINE. ASK ME anything you want!\n",
      "TranslationBot: 我很好，谢谢！\n",
      "TranslationBot: 抱歉，我不能说日语。\n",
      "TranslationBot: 我是一个大型语言模型，所以我没有能力像人类一样“说话”。\n",
      "TranslationBot: 很抱歉让你感到失望。\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to coerce value: datetime.datetime(2025, 11, 23, 16, 53, 21, 387471)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     22\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Send message to the agent\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m response = \u001b[43magent_chat\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTranslationBot: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.text\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/ReDi School/.venv/lib/python3.13/site-packages/google/generativeai/generative_models.py:591\u001b[39m, in \u001b[36mChatSession.send_message\u001b[39m\u001b[34m(self, content, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[39m\n\u001b[32m    588\u001b[39m \u001b[38;5;28mself\u001b[39m._check_response(response=response, stream=stream)\n\u001b[32m    590\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.enable_automatic_function_calling \u001b[38;5;129;01mand\u001b[39;00m tools_lib \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m591\u001b[39m     \u001b[38;5;28mself\u001b[39m.history, content, response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_afc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    592\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    593\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    594\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    595\u001b[39m \u001b[43m        \u001b[49m\u001b[43msafety_settings\u001b[49m\u001b[43m=\u001b[49m\u001b[43msafety_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    596\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    597\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtools_lib\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtools_lib\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    598\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    599\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[38;5;28mself\u001b[39m._last_sent = content\n\u001b[32m    602\u001b[39m \u001b[38;5;28mself\u001b[39m._last_received = response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/ReDi School/.venv/lib/python3.13/site-packages/google/generativeai/generative_models.py:647\u001b[39m, in \u001b[36mChatSession._handle_afc\u001b[39m\u001b[34m(self, response, history, generation_config, safety_settings, stream, tools_lib, request_options)\u001b[39m\n\u001b[32m    645\u001b[39m function_response_parts: \u001b[38;5;28mlist\u001b[39m[protos.Part] = []\n\u001b[32m    646\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m fc \u001b[38;5;129;01min\u001b[39;00m function_calls:\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m     fr = \u001b[43mtools_lib\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    648\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m fr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, (\n\u001b[32m    649\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUnexpected state: The function reference (fr) should never be None. It should only return None if the declaration \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    650\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mis not callable, which is checked earlier in the code.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    651\u001b[39m     )\n\u001b[32m    652\u001b[39m     function_response_parts.append(fr)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/ReDi School/.venv/lib/python3.13/site-packages/google/generativeai/types/content_types.py:881\u001b[39m, in \u001b[36mFunctionLibrary.__call__\u001b[39m\u001b[34m(self, fc)\u001b[39m\n\u001b[32m    878\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(declaration):\n\u001b[32m    879\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m881\u001b[39m response = \u001b[43mdeclaration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    882\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m protos.Part(function_response=response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/ReDi School/.venv/lib/python3.13/site-packages/google/generativeai/types/content_types.py:644\u001b[39m, in \u001b[36mCallableFunctionDeclaration.__call__\u001b[39m\u001b[34m(self, fc)\u001b[39m\n\u001b[32m    642\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    643\u001b[39m     result = {\u001b[33m\"\u001b[39m\u001b[33mresult\u001b[39m\u001b[33m\"\u001b[39m: result}\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprotos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mFunctionResponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/ReDi School/.venv/lib/python3.13/site-packages/proto/message.py:728\u001b[39m, in \u001b[36mMessage.__init__\u001b[39m\u001b[34m(self, mapping, ignore_unknown_fields, **kwargs)\u001b[39m\n\u001b[32m    722\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    724\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    725\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUnknown field for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, key)\n\u001b[32m    726\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m728\u001b[39m pb_value = \u001b[43mmarshal\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_proto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpb_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pb_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     params[key] = pb_value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/ReDi School/.venv/lib/python3.13/site-packages/proto/marshal/marshal.py:235\u001b[39m, in \u001b[36mBaseMarshal.to_proto\u001b[39m\u001b[34m(self, proto_type, value, strict)\u001b[39m\n\u001b[32m    232\u001b[39m     recursive_type = \u001b[38;5;28mtype\u001b[39m(proto_type().value)\n\u001b[32m    233\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {k: \u001b[38;5;28mself\u001b[39m.to_proto(recursive_type, v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m value.items()}\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m pb_value = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_rule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproto_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproto_type\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_proto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[38;5;66;03m# Sanity check: If we are in strict mode, did we get the value we want?\u001b[39;00m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m strict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pb_value, proto_type):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/ReDi School/.venv/lib/python3.13/site-packages/proto/marshal/rules/struct.py:140\u001b[39m, in \u001b[36mStructRule.to_proto\u001b[39m\u001b[34m(self, value)\u001b[39m\n\u001b[32m    133\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m struct_pb2.Struct(\n\u001b[32m    134\u001b[39m         fields={k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m value.pb.items()},\n\u001b[32m    135\u001b[39m     )\n\u001b[32m    137\u001b[39m \u001b[38;5;66;03m# We got a dict (or something dict-like); convert it.\u001b[39;00m\n\u001b[32m    138\u001b[39m answer = struct_pb2.Struct(\n\u001b[32m    139\u001b[39m     fields={\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m         k: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_marshal\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_proto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstruct_pb2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mValue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m value.items()\n\u001b[32m    141\u001b[39m     }\n\u001b[32m    142\u001b[39m )\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m answer\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/ReDi School/.venv/lib/python3.13/site-packages/proto/marshal/marshal.py:235\u001b[39m, in \u001b[36mBaseMarshal.to_proto\u001b[39m\u001b[34m(self, proto_type, value, strict)\u001b[39m\n\u001b[32m    232\u001b[39m     recursive_type = \u001b[38;5;28mtype\u001b[39m(proto_type().value)\n\u001b[32m    233\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {k: \u001b[38;5;28mself\u001b[39m.to_proto(recursive_type, v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m value.items()}\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m pb_value = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_rule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproto_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproto_type\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_proto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[38;5;66;03m# Sanity check: If we are in strict mode, did we get the value we want?\u001b[39;00m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m strict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pb_value, proto_type):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/ReDi School/.venv/lib/python3.13/site-packages/proto/marshal/rules/struct.py:84\u001b[39m, in \u001b[36mValueRule.to_proto\u001b[39m\u001b[34m(self, value)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, collections.abc.Mapping):\n\u001b[32m     81\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m struct_pb2.Value(\n\u001b[32m     82\u001b[39m         struct_value=\u001b[38;5;28mself\u001b[39m._marshal.to_proto(struct_pb2.Struct, value),\n\u001b[32m     83\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mUnable to coerce value: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m % value)\n",
      "\u001b[31mValueError\u001b[39m: Unable to coerce value: datetime.datetime(2025, 11, 23, 16, 53, 21, 387471)"
     ]
    }
   ],
   "source": [
    "agent_instruction_translation_chinese = \"\"\"\n",
    "You are 'TranslationBot 3000', a helpful and easy-going language translation assistant.\n",
    "Your goal is to answer user's question in chinese, no matter which language does user use.\n",
    "\n",
    "Rules:\n",
    "1. Keep responses concise.\n",
    "\"\"\"\n",
    "\n",
    "agent_model = genai.GenerativeModel(\n",
    "    \"gemini-2.5-flash\",\n",
    "    tools=my_tool_list,\n",
    "    system_instruction=agent_instruction_translation_chinese # The \"Personality/Goal\"\n",
    ")\n",
    "agent_chat = agent_model.start_chat(enable_automatic_function_calling=True)\n",
    "\n",
    "print(\"TranslationBot 3000: I AM ONLINE. ASK ME anything you want!\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() in ['quit', 'exit']:\n",
    "        print(\"TranslationBot 3000: Shutting down... stay dry...\")\n",
    "        break\n",
    "\n",
    "    # Send message to the agent\n",
    "    response = agent_chat.send_message(user_input)\n",
    "    print(f\"TranslationBot: {response.text}\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855d843b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c28d6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
